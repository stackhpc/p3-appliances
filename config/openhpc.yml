---
# This name is used for the Heat stack and as a prefix for the
# cluster node hostnames.
cluster_name: openhpc

# This parameter should be set to the name of an RSA keypair you have
# uplaoded to OpenStack.
cluster_keypair: bharat

# Site-specific network configuration.
cluster_net:
  - { net: "ilab", subnet: "ilab" }
  - { net: "p3-bdn", subnet: "p3-bdn" }
  - { net: "p3-lln", subnet: "p3-lln" }

# A 3-NIC node resource in the heat templates.
cluster_nodenet_environment: "nodenet-3.yaml"

# Enable the use of config drive, for managing IP assignment on IPoIB
cluster_config_drive: true

# Multi-node application topology.  In this case we have a SLURM
# deployment formed from a login/controller node and a number of
# batch compute nodes.
cluster_groups:
  - "{{ slurm_login }}"
  - "{{ slurm_compute }}"

slurm_login:
  name: "login"
  flavor: "compute-B"
  image: "CentOS7.5-OpenHPC"
  num_nodes: 1
  user: "centos"

slurm_compute:
  name: "compute"
  flavor: "compute-A"
  image: "CentOS7.5-OpenHPC"
  num_nodes: 8
  user: "centos"

# Node group assignments for cluster roles.
# These group assignments are appended to the cluster inventory file.
# The names of these roles are cross-referenced to groups referred to
# in playbooks in the ansible/ directory.
cluster_roles:
  - name: "ceph_client"
    groups: "{{ cluster_groups }}"
  - name: "login"
    groups: [ "{{ slurm_login }}" ]
  - name: "batch"
    groups: [ "{{ slurm_compute }}" ]
  - name: "mdadm"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_server"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_client"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_mgmt"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_mds"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_oss"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_client"
    groups:
    - "{{ slurm_login }}"
    - "{{ slurm_compute }}" 

# Define a list of SLURM partitions to create.
openhpc_slurm_partitions: 
  - "{{ slurm_compute }}"

# A list of OpenHPC runtime libraries to install on compute and control nodes
openhpc_packages:
  - strace
  - flex
  - bison
  - blas
  - blas-devel
  - lapack
  - lapack-devel
  - cfitsio
  - cfitsio-devel
  - wcslib
  - wcslib-utils
  - wcslib-devel
  - gcc-gfortran 
  - gcc-c++
  - ncurses
  - ncurses-devel
  - readline
  - readline-devel
  - python-devel
  - boost
  - boost-devel
  - fftw
  - fftw-devel
  - hdf5
  - hdf5-devel
  - numpy
  - boost-python
  - hdf5-gnu-ohpc
  - phdf5-gnu-mvapich2-ohpc
  - gnu-compilers-ohpc
  - mvapich2-gnu-ohpc
  - openmpi-gnu-ohpc
  - imb-gnu-mvapich2-ohpc
  - imb-gnu-openmpi-ohpc
  - openblas-gnu-ohpc
  - scalapack-gnu-mvapich2-ohpc
  - python34
  - python34-devel
  - python-virtualenv
  - infiniband-diags
  - emacs
#  - git-lfs

# Choose between glusterfs and beegfs
cluster_fs: glusterfs

# Gluster config
gluster_cluster_volume_name: openhpc
gluster_cluster_block_devices:
- md0
gluster_cluster_transport_interface: ib0
gluster_cluster_transport_mode: rdma
gluster_cluster_force_format: no
gluster_cluster_volume_options:
  cluster.nufa: 'on'
  performance.cache-size: '10GB'
  diagnostics.brick-log-level: 'WARNING'

gluster_src: "localhost:/{{ gluster_cluster_volume_name }}"
gluster_mnt: /mnt/gluster

# Config for Ceph mount
ceph_mount_share_name: HomeDirs
ceph_mount_path: /alaska
ceph_mount_fuse: true

# BeeGFS Configuration
beegfs_kmod_preload:
  - "mlx5_core"
  - "mlx5_ib"
  - "ib_ipoib"

# Software defined raid config
md0:
  # Define array name
  name: 'md0'
  # Define disk devices to assign to array
  devices:
    - '/dev/sdb'
    - '/dev/sdc'
    - '/dev/sdd'
  # Define filesystem to partition array with
  filesystem: 'xfs'
  filesystem_opts: ''
  # Define the array raid level
  # 0|1|4|5|6|10
  level: '0'
  # Define if array should be present or absent
  state: 'present'

mdadm_arrays:
  - "{{ md0 }}"
...
