---
# This name is used for the Heat stack and as a prefix for the
# cluster node hostnames.
cluster_name: openhpc-nvme

# This parameter should be set to the name of an RSA keypair you have
# uplaoded to OpenStack.
cluster_keypair: bharat

# Site-specific network configuration.
cluster_net:
  - { net: "ilab", subnet: "ilab" }
  - { net: "p3-bdn", subnet: "p3-bdn" }
  - { net: "p3-lln", subnet: "p3-lln" }

# A 3-NIC node resource in the heat templates.
cluster_nodenet_environment: "nodenet-3.yaml"

# Enable the use of config drive, for managing IP assignment on IPoIB
cluster_config_drive: true

# Multi-node application topology.  In this case we have a SLURM
# deployment formed from a login/controller node and a number of
# batch compute nodes.
cluster_groups:
  - "{{ storage }}"

storage:
  name: "storage"
  flavor: "storage-A"
  image: "CentOS7.5-OpenHPC"
  num_nodes: 1
  user: "centos"

# Node group assignments for cluster roles.
# These group assignments are appended to the cluster inventory file.
# The names of these roles are cross-referenced to groups referred to
# in playbooks in the ansible/ directory.
cluster_roles:
  - name: "ceph_client"
    groups: "{{ cluster_groups }}"
  - name: "glusterfs_server"
    groups: [ "{{ storage }}" ]
  - name: "glusterfs_client"
    groups: [ "{{ storage }}" ]
  - name: "beegfs_oss"
    groups: [ "{{ storage }}" ]
  - name: "beegfs_client"
    groups: ["{{ storage }}" ]

# A list of OpenHPC runtime libraries to install on compute and control nodes
openhpc_packages:
  - strace
  - flex
  - bison
  - blas
  - blas-devel
  - lapack
  - lapack-devel
  - cfitsio
  - cfitsio-devel
  - wcslib
  - wcslib-utils
  - wcslib-devel
  - gcc-gfortran 
  - gcc-c++
  - ncurses
  - ncurses-devel
  - readline
  - readline-devel
  - python-devel
  - boost
  - boost-devel
  - fftw
  - fftw-devel
  - hdf5
  - hdf5-devel
  - numpy
  - boost-python
  - hdf5-gnu-ohpc
  - phdf5-gnu-mvapich2-ohpc
  - gnu-compilers-ohpc
  - mvapich2-gnu-ohpc
  - openmpi-gnu-ohpc
  - imb-gnu-mvapich2-ohpc
  - imb-gnu-openmpi-ohpc
  - openblas-gnu-ohpc
  - scalapack-gnu-mvapich2-ohpc
  - python34
  - python34-devel
  - python-virtualenv
  - infiniband-diags
  - emacs
#  - git-lfs

# Choose between glusterfs and beegfs
cluster_fs: glusterfs

# Gluster config
gluster_cluster_volume_name: openhpc_nvme
gluster_cluster_block_devices:
- nvme0n1
- nvme1n1
- nvme2n1
- nvme3n1
gluster_cluster_transport_interface: ib0
gluster_cluster_transport_mode: rdma
gluster_cluster_force_format: yes
gluster_cluster_volume_options:
  performance.cache-size: '10GB'
  diagnostics.brick-log-level: 'WARNING'

gluster_src: "localhost:/{{ gluster_cluster_volume_name }}"
gluster_mnt: /mnt/gluster_nvme

# Config for Ceph mount
ceph_mount_share_name: HomeDirs
ceph_mount_path: /alaska
ceph_mount_fuse: true

# BeeGFS Configuration
beegfs_kmod_preload:
  - "mlx5_core"
  - "mlx5_ib"
  - "ib_ipoib"
...
